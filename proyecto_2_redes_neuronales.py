# -*- coding: utf-8 -*-
"""Proyecto_2_Redes_Neuronales.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PYy-anQ4OYuILjHlvg4-45uMl2s2YEbT
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.model_selection import train_test_split
from google.colab import drive
drive.mount("/content/drive")

data = pd.read_csv("/content/drive/MyDrive/Consulta_hospitales.csv", encoding ='ISO-8859-1')

data.head()

data = data.drop("source_file", axis=1, errors='ignore')
data = data.rename(columns={
    "ppertenencia": "pueblo_pertenencia",
    "periodoeda": "periodo_edad",
    "muniresiden": "municipio_residencia",
    "tc": "tipo_consulta",
    "caufin": "causa_atencion"
})

data = data.rename(columns={
    "ppertenencia": "pueblo_pertenencia",
    "periodoeda": "periodo_edad",
    "muniresiden": "municipio_residencia",
    "tc": "tipo_consulta",
    "caufin": "causa_atencion"
})

data.head()

"""# **Caso 1: Consultas de Emergencia en la Capital**

"""

datos_mayores_emergencias = data[
    (data["tipo_consulta"] == 3) &
    (data["deptoresiden"] == 1) &
    (data["causa_atencion"] != "A099")
]

x = datos_mayores_emergencias[['edad','mes','sexo','deptoresiden']]
x = x.fillna(0)
y, _ = pd.factorize(datos_mayores_emergencias['causa_atencion'])

x_train, x_test, y_train ,y_test = train_test_split(x,y, test_size=0.2, random_state=42)

num_classes = len(np.unique(y))
model = Sequential()
model.add(Dense(16, input_dim=3, activation='relu'))
model.add(Dense(16,activation='relu'))
model.add(Dense(num_classes, activation='softmax'))

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(x_train, y_train, epochs=50, batch_size=64, validation_data=(x_test, y_test))

loss, accuracy = model.evaluate(x_test, y_test)
print(f'Loss: {loss}, Accuracy: {accuracy}')

#Perfil 1: Mujer de 25 años, enferma en octubre
Perfil1 = np.array([[25,10,2]])
res = model.predict(Perfil1)
print(res)

#Perfil 2: Hombre de 10 años, enfermo en julio
Perfil2 = np.array([[10,7,1]])
res = model.predict(Perfil1)
print(res)

#Perfil 3: Mujer de 5 años, enferma en enero
Perfil3 = np.array([[5,1,2]])
res = model.predict(Perfil1)
print(res)

"""# Caso 2: Diagnóstico de Acné Vulgar y Asociación con Pueblo de Pertenencia

Sin balancear los datos, se obtuvo un desbalanceo, ya que la mayoría de datos eran No_L700
"""

datos_xinka = data[['causa_atencion', 'sexo', 'pueblo_pertenencia', 'tipo_consulta']].copy()

datos_xinka['causa'] = np.where(datos_xinka['causa_atencion'] == "L700", 1, 0)
datos_xinka = datos_xinka.dropna()

datos_xinka =datos_xinka.sample(frac=1, random_state=42).reset_index(drop='TRUE')

x = datos_xinka[['pueblo_pertenencia','sexo','tipo_consulta']]
x = x.fillna(0)
y = datos_xinka['causa']

x_train, x_test, y_train ,y_test = train_test_split(x,y, test_size=0.2, random_state=42)

model = Sequential()
model.add(Dense(8, input_dim=3, activation='relu'))
model.add(Dense(4,activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(x_train, y_train, epochs=50, batch_size=64, validation_data=(x_test, y_test))

loss, accuracy = model.evaluate(x_test, y_test)
print(f'Loss: {loss}, Accuracy: {accuracy}')

"""Se balancearon los datos debido que en general, los casos de Acné vulgar eran casi 0 en todo el dataset"""

idx_1 = datos_xinka[datos_xinka['causa'] == 1].index   # Si_L700
idx_0 = datos_xinka[datos_xinka['causa'] == 0].index   # No_L700

n_1 = len(idx_1)
n_0_mostrar = min(len(idx_0), n_1 * 4)

np.random.seed(123)
idx_0_sub = np.random.choice(idx_0, n_0_mostrar, replace=False)

datos_bal = datos_xinka.loc[np.concatenate([idx_1, idx_0_sub])]

# mezclar filas
datos_bal = datos_bal.sample(frac=1, random_state=42).reset_index(drop=True)

print(datos_bal['causa'].value_counts())

X = datos_bal[['pueblo_pertenencia', 'sexo', 'tipo_consulta']]
y = datos_bal['causa']  # 0 / 1

X = X.fillna(0)

x_train, x_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = Sequential()
model.add(Dense(8, input_dim=3, activation='relu'))
model.add(Dense(4, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(x_train, y_train, epochs=50, batch_size=64, validation_data=(x_test, y_test))

loss, acc = model.evaluate(x_test, y_test)
print("Loss:", loss, "Accuracy:", acc)

#Perfil 1: Mujer del pueblo Xinka, en reconsulta
Perfil1 = np.array([[3,2,2]])
res = model.predict(Perfil1)
print(res)

#Perfil 2: Hombre del pueblo Xinka, en reconsulta
Perfil2 = np.array([[3,1,2]])
res = model.predict(Perfil2)
print(res)

#Perfil 3: Mujer maya, en reconsulta
Perfil3 = np.array([[1,2,2]])
res = model.predict(Perfil3)
print(res)

"""# **Caso 3: Factores Sociodemográficos Asociados a Consultas de Emergencia**"""

datos_emergencia = data[['deptoresiden', 'sexo', 'pueblo_pertenencia', 'tipo_consulta']].copy()

datos_emergencia['consulta'] = np.where(datos_emergencia['tipo_consulta'] == 3, 1, 0)
datos_emergencia = datos_emergencia.dropna()

idx_1 = datos_emergencia[datos_emergencia['consulta'] == 1].index
idx_0 = datos_emergencia[datos_emergencia['consulta'] == 0].index

n_1 = len(idx_1)
n_0_mostrar = min(len(idx_0), n_1 * 4)

np.random.seed(123)
idx_0_sub = np.random.choice(idx_0, n_0_mostrar, replace=False)

datos_bal2 = datos_emergencia.loc[np.concatenate([idx_1, idx_0_sub])]

# mezclar filas
datos_bal2 = datos_bal2.sample(frac=1, random_state=42).reset_index(drop=True)

print(datos_bal2['consulta'].value_counts())

X = datos_bal2[['deptoresiden', 'pueblo_pertenencia', 'sexo']]
y = datos_bal2['consulta']  # 0 / 1

X = X.fillna(0)

x_train, x_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = Sequential()
model.add(Dense(8, input_dim=3, activation='relu'))
model.add(Dense(4, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(x_train, y_train, epochs=50, batch_size=64, validation_data=(x_test, y_test))

loss, acc = model.evaluate(x_test, y_test)
print("Loss:", loss, "Accuracy:", acc)

#Perfil 1: Hombre ladino de Santa Rosa
Perfil1 = np.array([[6,4,1]])
res = model.predict(Perfil1)
print(res)

#Perfil 2: Mujer maya de Sacatepéquez
Perfil2 = np.array([[3,1,2]])
res = model.predict(Perfil2)
print(res)

#Perfil 3: Mujer ladina de Petén
Perfil3 = np.array([[17,4,2]])
res = model.predict(Perfil3)
print(res)